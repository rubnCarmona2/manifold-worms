{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from typing import *\n",
    "import wandb\n",
    "from mw import log\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifoldWorms(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        env_dim: int,\n",
    "    ):\n",
    "        super(ManifoldWorms, self).__init__()\n",
    "        self.positions = nn.ParameterDict(\n",
    "            {\n",
    "                \"input_tails\": nn.Parameter(\n",
    "                    torch.randn(input_size, env_dim), requires_grad=True\n",
    "                ),\n",
    "                \"exit_heads\": nn.Parameter(\n",
    "                    torch.randn(output_size, env_dim), requires_grad=True\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.normalize_positions()\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        self.normalize_positions()\n",
    "        similarities = self.positions[\"exit_heads\"] @ self.positions[\"input_tails\"].T\n",
    "        return similarities\n",
    "\n",
    "    def normalize_positions(self):\n",
    "        for name in self.positions:\n",
    "            self.positions[name].data.copy_(\n",
    "                F.normalize(self.positions[name], p=2, dim=1).data\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, n_features: int = 4):\n",
    "        self.data = torch.randn(256, n_features, 1)\n",
    "        self.data /= self.data.abs().max()\n",
    "        self.data[:, 0] = 1\n",
    "        self.label = torch.ones(256, 1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 256\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BaseDataset()\n",
    "test_dataset = BaseDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = torch.Tensor(X).unsqueeze(-1)\n",
    "        self.data /= self.data.abs().max()\n",
    "        self.label = torch.Tensor(y).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.label /= self.label.abs().max()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_samples=1_000, n_features=12, noise=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### California Housing\n",
    "###### * Requires deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = SklearnDataset(X_train, y_train)\n",
    "test_dataset = SklearnDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "USE_WANDB = True\n",
    "n_features = train_dataloader.dataset[0][0].shape[0]\n",
    "env_dims = 16\n",
    "l1_scale = 0.2\n",
    "weight_radius = 0.2\n",
    "\n",
    "model = ManifoldWorms(n_features, 1, env_dims)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\TrabalhosEstudos\\IA\\ManifoldWorms\\wandb\\run-20250204_112922-wjn1bkmz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rubn/manifold_worms/runs/wjn1bkmz' target=\"_blank\">morning-fire-37</a></strong> to <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">https://wandb.ai/rubn/manifold_worms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rubn/manifold_worms/runs/wjn1bkmz' target=\"_blank\">https://wandb.ai/rubn/manifold_worms/runs/wjn1bkmz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>▃▄▇▅▇▄▅▄▆▆▄▂▄▅▂▄▄▄▅▃▂▄▃▃█▄▃▆▄▂▅▃▃▄▃▁▆▆▇▂</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>0.22344</td></tr><tr><td>train_loss</td><td>0.2249</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-fire-37</strong> at: <a href='https://wandb.ai/rubn/manifold_worms/runs/wjn1bkmz' target=\"_blank\">https://wandb.ai/rubn/manifold_worms/runs/wjn1bkmz</a><br> View project at: <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">https://wandb.ai/rubn/manifold_worms</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250204_112922-wjn1bkmz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run = wandb.init(project=\"manifold_worms\")\n",
    "\n",
    "logs = defaultdict(list)\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    logs[\"train\"].clear()\n",
    "    for X, y in train_dataloader:\n",
    "        X = X[0].requires_grad_(True)\n",
    "        y = y[0]\n",
    "        weights = model()\n",
    "        y_pred = weights @ X\n",
    "        loss = F.mse_loss(y_pred, y) + l1_scale * weights.abs().sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logs[\"train\"].append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    logs[\"test\"].clear()\n",
    "    for X, y in test_dataloader:\n",
    "        X = X[0].requires_grad_(True)\n",
    "        y = y[0]\n",
    "        weights = model()\n",
    "        y_pred = weights @ X\n",
    "        loss = F.mse_loss(y_pred, y) + l1_scale * weights.abs().sum()\n",
    "        logs[\"test\"].append(loss.item())\n",
    "\n",
    "    if USE_WANDB:\n",
    "        run.log(\n",
    "            {\n",
    "                \"train_loss\" : sum(logs[\"train\"]) / len(logs[\"train\"]),\n",
    "                \"test_loss\" : sum(logs[\"test\"]) / len(logs[\"test\"])\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"train_loss\", sum(logs[\"train\"]) / len(logs[\"train\"]),\n",
    "              \"\\ntest loss\", sum(logs[\"test\"]) / len(logs[\"test\"]))\n",
    "    \n",
    "    logs[\"state\"].append(\n",
    "        log.visualize(\n",
    "            model.positions[\"input_tails\"].data,\n",
    "            model.positions[\"exit_heads\"].data\n",
    "        )\n",
    "    )\n",
    "\n",
    "if USE_WANDB:\n",
    "    run.log(\n",
    "        {\n",
    "        \"video\" : wandb.Video(\n",
    "            np.stack(logs[\"state\"]).transpose(0, 3, 1, 2),\n",
    "            fps=15,\n",
    "            format=\"gif\"\n",
    "        )\n",
    "        }\n",
    "    )\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
