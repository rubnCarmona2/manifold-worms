{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mw\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_max, X_min = X.max(0), X.min(0)\n",
    "y_max, y_min = y.max(0), y.min(0)\n",
    "X = (X - X_min) / (X_max - X_min)\n",
    "y = (y - y_min) / (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = torch.Tensor(X).unsqueeze(-1)\n",
    "        self.label = torch.Tensor(y).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = SklearnDataset(X_train, y_train)\n",
    "test_dataset = SklearnDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_grads(model, eps=1e-6):\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.div_(param.grad.norm().clip(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x2779a75fb00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE = 8\n",
    "HIDDEN_SIZE = 8\n",
    "N_HIDDEN_LAYERS = 2\n",
    "OUTPUT_SIZE = 1\n",
    "DIMS = 6\n",
    "\n",
    "model_base = mw.models.Regression(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    n_hidden_layers=N_HIDDEN_LAYERS,\n",
    "    output_size=OUTPUT_SIZE\n",
    ")\n",
    "model_mw = mw.models.ManifoldWorms(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE * N_HIDDEN_LAYERS,\n",
    "    output_size=1,\n",
    "    d=6\n",
    ")\n",
    "\n",
    "optim_base = optim.AdamW(model_base.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "optim_mw = optim.AdamW(model_mw.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "optim_mw.register_step_post_hook(model_mw.post_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WANDB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrubn\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\TrabalhosEstudos\\IA\\ManifoldWorms\\wandb\\run-20250212_182818-yl9aypxw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rubn/manifold_worms/runs/yl9aypxw' target=\"_blank\">polished-grass-55</a></strong> to <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">https://wandb.ai/rubn/manifold_worms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rubn/manifold_worms/runs/yl9aypxw' target=\"_blank\">https://wandb.ai/rubn/manifold_worms/runs/yl9aypxw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rubn\\AppData\\Local\\Temp\\ipykernel_13380\\2207492531.py:72: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1808.)\n",
      "  scalars[f\"{model}_grad_{name}_std\"] = param.grad.std().item()\n"
     ]
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run = wandb.init(project=\"manifold_worms\")\n",
    "\n",
    "logs = defaultdict(list)\n",
    "for epoch in range(100):\n",
    "\n",
    "    for k in logs:\n",
    "        if any([x in k for x in [\"train\", \"test\"]]):\n",
    "            logs[k].clear()\n",
    "\n",
    "    model_mw.train()\n",
    "    model_base.train()\n",
    "    for X, y in train_dataloader:\n",
    "\n",
    "        # mw training\n",
    "        model_mw.clear_state()\n",
    "        y_pred_mw = model_mw(X)\n",
    "        for _ in range(100):\n",
    "            increment = model_mw()\n",
    "            y_pred_mw = y_pred_mw + increment\n",
    "            if increment.norm() < 1e-4:\n",
    "                break\n",
    "\n",
    "        rmse_loss = F.mse_loss(y_pred_mw, y).sqrt()\n",
    "        garbage_loss = model_mw.state.mean(0).abs().sum()\n",
    "        loss_mw = rmse_loss + garbage_loss\n",
    "        logs[\"mw_train_loss\"].append(loss_mw.item())\n",
    "        optim_mw.zero_grad()\n",
    "        loss_mw.backward()\n",
    "        normalize_grads(model_mw)\n",
    "        optim_mw.step()\n",
    "\n",
    "        # baseline training\n",
    "        y_pred_base = model_base(X[..., 0])\n",
    "        loss_base = F.mse_loss(y_pred_base, y[..., 0]).sqrt()\n",
    "        logs[\"base_train_loss\"].append(loss_base.item())\n",
    "        optim_base.zero_grad()\n",
    "        loss_base.backward()\n",
    "        optim_base.step()\n",
    "\n",
    "    model_mw.eval()\n",
    "    model_base.eval()\n",
    "    for X, y in test_dataloader:\n",
    "\n",
    "        # mw eval\n",
    "        model_mw.clear_state()\n",
    "        y_pred_mw = model_mw(X)\n",
    "        for _ in range(100):\n",
    "            increment = model_mw()\n",
    "            if increment.norm() < 1e-4:\n",
    "                break\n",
    "            y_pred_mw = y_pred_mw + increment\n",
    "        rmse_loss = F.mse_loss(y_pred_mw, y).sqrt()\n",
    "        garbage_loss = model_mw.state.mean(0).abs().sum()\n",
    "        loss_mw = rmse_loss + garbage_loss\n",
    "        logs[\"mw_test_loss\"].append(loss_mw.item())\n",
    "\n",
    "        # baseline eval\n",
    "        y_pred_base = model_base(X[..., 0])\n",
    "        loss_base = F.mse_loss(y_pred_base, y[..., 0]).sqrt()\n",
    "        logs[\"base_test_loss\"].append(loss_base.item())\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        scalars = {\n",
    "            key : sum(values) / len(values)\n",
    "            for key, values in logs.items() if key != \"state\"\n",
    "        }\n",
    "        for model, name in [(model_mw, 'mw'), (model_base, 'base')]:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    scalars[f\"{model}_grad_{name}_mean\"] = param.grad.mean().item()\n",
    "                    scalars[f\"{model}_grad_{name}_std\"] = param.grad.std().item()\n",
    "        run.log(scalars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
