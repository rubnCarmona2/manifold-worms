{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1fd80562cc0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from typing import *\n",
    "import wandb\n",
    "from mw import log\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifoldWorms(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        env_dim: int,\n",
    "    ):\n",
    "        super(ManifoldWorms, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.hidden_state = torch.zeros(input_size + hidden_size, 1).requires_grad_(True)\n",
    "        self.outputs_mask = torch.zeros(output_size + hidden_size, 1)\n",
    "        self.outputs_mask[: output_size] = 1\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(output_size + hidden_size, 1))\n",
    "        self.positions = nn.ParameterDict(\n",
    "            {\n",
    "                \"tails\": nn.Parameter(\n",
    "                    torch.randn(input_size + hidden_size, env_dim), requires_grad=True\n",
    "                ),\n",
    "                \"heads\": nn.Parameter(\n",
    "                    torch.randn(output_size + hidden_size, env_dim), requires_grad=True\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            max_loops: int = 20,\n",
    "            with_empty_hidden_state: bool = True,\n",
    "            eps: float = 1e-4\n",
    "            ) -> torch.Tensor:\n",
    "        \n",
    "        assert x.shape[-2] == self.input_size\n",
    "        \n",
    "        if with_empty_hidden_state:\n",
    "            self.clear_hidden_state()\n",
    "        \n",
    "        x = F.pad(x, (0, 0, 0, self.hidden_size))\n",
    "        self.hidden_state = self.hidden_state + x\n",
    "\n",
    "        y = torch.zeros(self.output_size + self.hidden_size, 1)\n",
    "\n",
    "        self.normalize_positions()\n",
    "        similarities = self.positions[\"heads\"] @ self.positions[\"tails\"].T\n",
    "\n",
    "        for _ in range(max_loops):\n",
    "            # core transformations\n",
    "            new_hidden_state = similarities @ self.hidden_state\n",
    "            new_hidden_state = new_hidden_state + self.bias\n",
    "            new_hidden_state = F.tanh(new_hidden_state)\n",
    "\n",
    "            # move output head's inputs out of the loop\n",
    "            y = y + new_hidden_state * self.outputs_mask\n",
    "            new_hidden_state = new_hidden_state * (1 - self.outputs_mask)\n",
    "\n",
    "            # reshapes the outputs as a new input\n",
    "            new_hidden_state = F.pad(new_hidden_state, (0, 0, self.input_size - self.output_size, 0))\n",
    "            self.hidden_state = new_hidden_state\n",
    "\n",
    "            if new_hidden_state.norm() < eps:\n",
    "                break\n",
    "        \n",
    "        # Outputting similarities for L1 Regularization\n",
    "        return y[: self.output_size], similarities\n",
    "\n",
    "    def clear_hidden_state(self):\n",
    "        self.hidden_state = torch.zeros_like(self.hidden_state).requires_grad_(True)\n",
    "\n",
    "    def normalize_positions(self):\n",
    "        for name in self.positions:\n",
    "            self.positions[name].data.copy_(\n",
    "                F.normalize(self.positions[name].data, p=2, dim=1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, n_features: int = 4):\n",
    "        self.data = torch.randn(256, n_features, 1)\n",
    "        self.data /= self.data.abs().max()\n",
    "        self.data[:, 0] = 1\n",
    "        self.label = torch.ones(256, 1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 256\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BaseDataset()\n",
    "test_dataset = BaseDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        X = (X - X.mean(0)) / X.std(0)\n",
    "        y = (y - y.mean(0)) / y.std(0)\n",
    "        self.data = torch.Tensor(X).unsqueeze(-1)\n",
    "        self.label = torch.Tensor(y).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_samples=1_000, n_features=12, noise=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### California Housing (Requires deep NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = SklearnDataset(X_train, y_train)\n",
    "test_dataset = SklearnDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "USE_WANDB = True\n",
    "GRADIENT_NORM = True\n",
    "n_features = train_dataloader.dataset[0][0].shape[0]\n",
    "hidden_size = 10\n",
    "env_dims = 3\n",
    "l1_scale = 0.0\n",
    "\n",
    "model = ManifoldWorms(n_features, 1, hidden_size, env_dims)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\TrabalhosEstudos\\IA\\ManifoldWorms\\wandb\\run-20250205_170940-vhjngzgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rubn/manifold_worms/runs/vhjngzgf' target=\"_blank\">glorious-sunset-49</a></strong> to <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">https://wandb.ai/rubn/manifold_worms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rubn/manifold_worms/runs/vhjngzgf' target=\"_blank\">https://wandb.ai/rubn/manifold_worms/runs/vhjngzgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run = wandb.init(project=\"manifold_worms\")\n",
    "\n",
    "logs = defaultdict(list)\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for k in logs:\n",
    "        if \"test\" in k or \"train\" in k:\n",
    "            logs[k].clear()\n",
    "\n",
    "    model.train()\n",
    "    for X, y in train_dataloader:\n",
    "\n",
    "        X = X[0].requires_grad_(True)\n",
    "        y = y[0]\n",
    "        y_pred, cos_sim = model(X)\n",
    "\n",
    "        mse_loss = F.mse_loss(y_pred, y)\n",
    "        l1_loss = l1_scale * cos_sim.abs().sum()\n",
    "        garbage_loss = model.hidden_state.abs().sum()\n",
    "        loss = mse_loss + l1_loss + garbage_loss\n",
    "\n",
    "        logs[\"train_mse_loss\"].append(mse_loss.item())\n",
    "        logs[\"train_l1_loss\"].append(l1_loss.item())\n",
    "        logs[\"train_garbage_loss\"].append(garbage_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if GRADIENT_NORM:\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.div_(\n",
    "                        param.grad.norm().clip(1e-6)\n",
    "                    )\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    for X, y in test_dataloader:\n",
    "        X = X[0].requires_grad_(True)\n",
    "        y = y[0]\n",
    "        y_pred, cos_sim = model(X)\n",
    "\n",
    "        mse_loss = F.mse_loss(y_pred, y)\n",
    "        l1_loss = l1_scale * cos_sim.abs().sum()\n",
    "        garbage_loss = model.hidden_state.abs().sum()\n",
    "        loss = mse_loss + l1_loss + garbage_loss\n",
    "\n",
    "        logs[\"test_mse_loss\"].append(mse_loss.item())\n",
    "        logs[\"test_l1_loss\"].append(l1_loss.item())\n",
    "        logs[\"test_garbage_loss\"].append(garbage_loss.item())\n",
    "\n",
    "    logs[\"state\"].append(\n",
    "        log.visualize(\n",
    "            model.positions[\"tails\"].data,\n",
    "            model.positions[\"heads\"].data\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        scalars = {\n",
    "            key : sum(values) / len(values)\n",
    "            for key, values in logs.items() if key != \"state\"\n",
    "        }\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                scalars[f\"grad_{name}_mean\"] = param.grad.mean().item()\n",
    "                scalars[f\"grad_{name}_std\"] = param.grad.std().item()\n",
    "        run.log(scalars)\n",
    "\n",
    "if USE_WANDB:\n",
    "    run.log(\n",
    "        {\n",
    "        \"video\" : wandb.Video(\n",
    "            np.stack(logs[\"state\"]).transpose(0, 3, 1, 2),\n",
    "            fps=15,\n",
    "            format=\"gif\"\n",
    "        )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>grad_bias_mean</td><td>▇▅▆▇▁▇█▂▇▂█▂▆▃▆▂▆▅▅▁▃▆▇▆▄▄▄▃▄▅▅▅▃▂▄▅▂▄▄▄</td></tr><tr><td>grad_bias_std</td><td>▆█▆▄▃▅▁▅▄▅▁▆▇█▇▆▆█▇▄█▆█▅▆██▇█▇▇▇▅█▆█▇███</td></tr><tr><td>grad_positions.heads_mean</td><td>▃▇▂▂█▁█▂█▃▇▃▅▅▄▆█▅▄▆▄▅▅▆▄▅▆▇▅▄▅▅▄▆▆▆▆▅▅▄</td></tr><tr><td>grad_positions.heads_std</td><td>▅█▇▃▃▅▁▅▄▅▁▆▇██▇▄█████████▇█████████▇███</td></tr><tr><td>grad_positions.tails_mean</td><td>▄▅▄▄▅▄▅▃▅▆▆▆█▁█▂▁▆█▄▆▃█▃▄▆▃▅▆█▆▆▆▇▁▂▂▂█▅</td></tr><tr><td>grad_positions.tails_std</td><td>█████████▇▇▅▅▂▄▃▇▂█▄▇▁▇█▅▇█▇▃▇▇▆▃▄█▆▆▁▇█</td></tr><tr><td>test_garbage_loss</td><td>██▇▇▆▆▆▆▆▆▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_l1_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_mse_loss</td><td>███▇▆▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_garbage_loss</td><td>███▇▇▇▆▆▆▆▆▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_l1_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mse_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>grad_bias_mean</td><td>-0.0063</td></tr><tr><td>grad_bias_std</td><td>0.31616</td></tr><tr><td>grad_positions.heads_mean</td><td>-0.01503</td></tr><tr><td>grad_positions.heads_std</td><td>0.17612</td></tr><tr><td>grad_positions.tails_mean</td><td>0.00746</td></tr><tr><td>grad_positions.tails_std</td><td>0.12381</td></tr><tr><td>test_garbage_loss</td><td>0.00189</td></tr><tr><td>test_l1_loss</td><td>0</td></tr><tr><td>test_mse_loss</td><td>0.00855</td></tr><tr><td>train_garbage_loss</td><td>0.00069</td></tr><tr><td>train_l1_loss</td><td>0</td></tr><tr><td>train_mse_loss</td><td>0.00451</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-silence-48</strong> at: <a href='https://wandb.ai/rubn/manifold_worms/runs/rfbdwjb0' target=\"_blank\">https://wandb.ai/rubn/manifold_worms/runs/rfbdwjb0</a><br> View project at: <a href='https://wandb.ai/rubn/manifold_worms' target=\"_blank\">https://wandb.ai/rubn/manifold_worms</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250205_140236-rfbdwjb0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2428]), tensor(-0.2976, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, y = next(iter(train_dataloader))\n",
    "y[0, 0], model(X)[0][0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
