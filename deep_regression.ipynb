{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1fd61e7aa50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from typing import *\n",
    "import wandb\n",
    "from mw import log\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifoldWorms(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        env_dim: int,\n",
    "    ):\n",
    "        super(ManifoldWorms, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.positions = nn.ParameterDict(\n",
    "            {\n",
    "                \"tails\": nn.Parameter(\n",
    "                    torch.randn(input_size + hidden_size, env_dim), requires_grad=True\n",
    "                ),\n",
    "                \"heads\": nn.Parameter(\n",
    "                    torch.randn(output_size + hidden_size, env_dim), requires_grad=True\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.normalize_positions()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        in_data = torch.zeros(self.input_size + self.hidden_size, 1)\n",
    "        in_data[: x.shape[0]] = x\n",
    "\n",
    "        out_data = torch.zeros(self.output_size, 1)\n",
    "\n",
    "        self.normalize_positions()\n",
    "        similarities = self.positions[\"heads\"] @ self.positions[\"tails\"].T\n",
    "\n",
    "        for _ in range(20):\n",
    "            x = similarities @ in_data\n",
    "            out_data = out_data + x[: self.output_size]\n",
    "            in_data[: self.output_size] = x[: self.output_size] * 0\n",
    "            if in_data.norm() < 1e-4:\n",
    "                break\n",
    "        \n",
    "        return out_data, similarities\n",
    "\n",
    "    def normalize_positions(self):\n",
    "        for name in self.positions:\n",
    "            self.positions[name].data.copy_(\n",
    "                F.normalize(self.positions[name], p=2, dim=1).data\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, n_features: int = 4):\n",
    "        self.data = torch.randn(256, n_features, 1)\n",
    "        self.data /= self.data.abs().max()\n",
    "        self.data[:, 0] = 1\n",
    "        self.label = torch.ones(256, 1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 256\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BaseDataset()\n",
    "test_dataset = BaseDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = torch.Tensor(X).unsqueeze(-1)\n",
    "        self.data /= self.data.abs().max()\n",
    "        self.label = torch.Tensor(y).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.label /= self.label.abs().max()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_samples=1_000, n_features=12, noise=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### California Housing\n",
    "###### * Requires deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = SklearnDataset(X_train, y_train)\n",
    "test_dataset = SklearnDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1_000\n",
    "USE_WANDB = True\n",
    "n_features = train_dataloader.dataset[0][0].shape[0]\n",
    "hidden_size = 20\n",
    "env_dims = 16\n",
    "l1_scale = 0.2\n",
    "weight_radius = 0.2\n",
    "\n",
    "model = ManifoldWorms(n_features, 1, hidden_size, env_dims)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (21x28 and 21x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m X \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 12\u001b[0m y_pred, cos_sim \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(y_pred, y) \u001b[38;5;241m+\u001b[39m l1_scale \u001b[38;5;241m*\u001b[39m cos_sim\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[65], line 37\u001b[0m, in \u001b[0;36mManifoldWorms.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m loops \u001b[38;5;241m=\u001b[39m [in_data]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43msimilarities\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloops\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     38\u001b[0m     out_data \u001b[38;5;241m=\u001b[39m out_data \u001b[38;5;241m+\u001b[39m x[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size]\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (21x28 and 21x1)"
     ]
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run = wandb.init(project=\"manifold_worms\")\n",
    "\n",
    "logs = defaultdict(list)\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    logs[\"train\"].clear()\n",
    "    for X, y in train_dataloader:\n",
    "        X = X[0].requires_grad_(True)\n",
    "        y = y[0]\n",
    "        y_pred, cos_sim = model(X)\n",
    "        loss = F.mse_loss(y_pred, y) + l1_scale * cos_sim.abs().sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logs[\"train\"].append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    logs[\"test\"].clear()\n",
    "    for X, y in test_dataloader:\n",
    "        X = X[0].requires_grad_(True)\n",
    "        y = y[0]\n",
    "        y_pred, cos_sim = model(X)\n",
    "        loss = F.mse_loss(y_pred, y) + l1_scale * cos_sim.abs().sum()\n",
    "        logs[\"test\"].append(loss.item())\n",
    "\n",
    "    if USE_WANDB:\n",
    "        run.log(\n",
    "            {\n",
    "                \"train_loss\" : sum(logs[\"train\"]) / len(logs[\"train\"]),\n",
    "                \"test_loss\" : sum(logs[\"test\"]) / len(logs[\"test\"])\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"train_loss\", sum(logs[\"train\"]) / len(logs[\"train\"]),\n",
    "              \"\\ntest loss\", sum(logs[\"test\"]) / len(logs[\"test\"]))\n",
    "    \n",
    "    logs[\"state\"].append(\n",
    "        log.visualize(\n",
    "            model.positions[\"input_tails\"].data,\n",
    "            model.positions[\"exit_heads\"].data\n",
    "        )\n",
    "    )\n",
    "\n",
    "if USE_WANDB:\n",
    "    run.log(\n",
    "        {\n",
    "        \"video\" : wandb.Video(\n",
    "            np.stack(logs[\"state\"]).transpose(0, 3, 1, 2),\n",
    "            fps=15,\n",
    "            format=\"gif\"\n",
    "        )\n",
    "        }\n",
    "    )\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
